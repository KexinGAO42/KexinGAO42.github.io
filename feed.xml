<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kexingao42.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kexingao42.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-08T00:17:57+00:00</updated><id>https://kexingao42.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Deep Learning with Keras from 0 to 1</title><link href="https://kexingao42.github.io/blog/2024/code/" rel="alternate" type="text/html" title="Deep Learning with Keras from 0 to 1"/><published>2024-04-02T00:00:00+00:00</published><updated>2024-04-02T00:00:00+00:00</updated><id>https://kexingao42.github.io/blog/2024/code</id><content type="html" xml:base="https://kexingao42.github.io/blog/2024/code/"><![CDATA[<p>Keras is an open-source neural network library written in Python, serving as a <strong>high-level interface</strong> for building and training deep learning models. It provides a user-friendly API that allows developers to quickly prototype and deploy neural networks with minimal boilerplate code. Keras supports various backend engines, including <strong>TensorFlow</strong>, <strong>Microsoft Cognitive Toolkit (CNTK)</strong>, and <strong>Theano</strong>, enabling seamless integration with existing deep learning frameworks. With its modular architecture, Keras facilitates rapid experimentation and model iteration.</p> <p>The <strong>Sequential API</strong> and <strong>Functional API</strong> are two different approaches for building and configuring neural networks in Keras.</p> <p>The <strong>Sequential API</strong> is simpler and more straightforward, primarily used for building sequential models where each layer has exactly one input tensor and one output tensor. This API is ideal for basic feedforward neural networks where data flows sequentially from one layer to the next.</p> <p>The <strong>Functional API</strong> offers more flexibility and supports more complex model architectures, including multi-input and multi-output networks, skip connections, and shared layers. It allows users to define a directed acyclic graph (DAG) of layers by explicitly connecting the output of one layer to the input of another. This API is preferred for building models with branching or merging layers, enabling the creation of sophisticated architectures such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and models with attention mechanisms.</p> <h1 id="sequential-api">Sequential API</h1> <h2 id="build-models">Build Models</h2> <p>Letâ€™s build some deep learning models using the sequential API.</p> <h3 id="deep-neural-network-dnn">Deep Neural Network (DNN)</h3> <p>A deep neural network (DNN) model using the Sequential API in Keras to classify handwritten digits from the MNIST dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Import necessary libraries
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="n">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="c1"># Step 2: Load and preprocess the dataset
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Step 3: Build the model using Sequential API
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Alternatively
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># View your model
</span><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

<span class="c1"># Step 4: Compile the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Step 5: Train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Step 6: Evaluate the model
</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Step 7: Predict
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># Predict for the first 5 test samples
</span><span class="n">predicted_classes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictions:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">predicted_classes</span><span class="p">)</span>
</code></pre></div></div> <h3 id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3> <p>Reason for Normalization</p> <p>Normalizing neural networks inputs improve our model. But deeper layers are trained based on previous layer outputs and since weights get updated via gradient descent, consecutive layers no longer benefit from normalization and they need to adapt to previous layersâ€™ weight changes, finding more trouble to learn their own weights. Batch normalization makes sure that, independently of the changes, the inputs to the next layers are normalized. It does this in a smart way, with trainable parameters that also learn how much of this normalization is kept scaling or shifting it.</p> <p>This improves gradient flow, allows for higher learning rates, reduces weight initializations dependence, adds regularization to our network and limits internal covariate shift; which is a funny name for a layerâ€™s dependence on the previous layer outputs when learning its weights. Batch normalization is widely used today in many deep learning models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Import necessary libraries
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="n">keras.datasets</span> <span class="kn">import</span> <span class="n">cifar10</span>
<span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="c1"># Step 2: Load and preprocess the dataset
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">cifar10</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Step 3: Build the CNN model with BatchNormalization
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
    <span class="nc">BatchNormalization</span><span class="p">(),</span>
    <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">BatchNormalization</span><span class="p">(),</span>
    <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">BatchNormalization</span><span class="p">(),</span>
    <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Step 4: Compile the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Step 5: Train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> 
          <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
          <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Step 6: Evaluate the model
</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h3 id="long-short-term-memory-lstm">Long-Short Term Memory (LSTM)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Import necessary libraries
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="n">keras.preprocessing</span> <span class="kn">import</span> <span class="n">sequence</span>

<span class="c1"># Step 2: Load and preprocess the dataset
</span><span class="n">max_features</span> <span class="o">=</span> <span class="mi">20000</span>  <span class="c1"># Number of words to consider as features
</span><span class="n">max_len</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Maximum length of reviews
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">.</span><span class="nf">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">sequence</span><span class="p">.</span><span class="nf">pad_sequences</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_len</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">sequence</span><span class="p">.</span><span class="nf">pad_sequences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_len</span><span class="p">)</span>

<span class="c1"># Step 3: Build the LSTM model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Embedding</span><span class="p">(</span><span class="n">max_features</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
    <span class="nc">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Step 4: Compile the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Step 5: Train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Step 6: Evaluate the model
</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Test accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h3 id="autoencoder-ae">Autoencoder (AE)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Define the dimensions of the input data
</span><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">784</span>  <span class="c1"># Assuming MNIST images of size 28x28 pixels
</span>
<span class="c1"># Define the encoder architecture
</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,)),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Define the decoder architecture
</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,)),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Combine the encoder and decoder into an autoencoder model
</span><span class="n">input_img</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,))</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">autoencoder</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">(</span><span class="n">input_img</span><span class="p">,</span> <span class="n">decoded</span><span class="p">)</span>

<span class="c1"># Compile the autoencoder model
</span><span class="n">autoencoder</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h1 id="functional-api">Functional API</h1> <p>The Keras Functional API allows you to create complex models with multiple inputs, multiple outputs, shared layers, and more. Unlike the Sequential API, where you add layers sequentially, in the Functional API, you explicitly connect layers to each other, which provides more flexibility in model architecture.</p> <h2 id="key-concepts">Key Concepts</h2> <p><strong>Input layers</strong>: Input layers are used to define the input shape of your data. In the Functional API, you explicitly define input layers.</p> <p><strong>Functional Connections</strong>: In the Functional API, you connect layers by passing the output of one layer as the input to another layer.</p> <p><strong>Model Creation</strong>: Once youâ€™ve defined the connections between layers, you create a <code class="language-plaintext highlighter-rouge">Model</code> by specifying the inputs and outputs.</p> <h2 id="use-cases">Use Cases</h2> <p>The Keras Functional API is well-suited for a variety of use cases where more flexibility and customization are required in defining neural network architectures. Some common use cases of the Keras Functional API include:</p> <ol> <li><strong>Multi-input and Multi-output Models</strong>: Functional API enables building models with multiple inputs and outputs, which is useful in scenarios like multi-task learning, where a single model can perform multiple related tasks simultaneously. For example, a model for image captioning could have an image input and a text input, and it could output a caption.</li> <li><strong>Models with Shared Layers</strong>: Functional API allows layers to be reused and shared between different parts of the model. This is beneficial when you want to apply the same operation to multiple inputs or when building Siamese networks for tasks like similarity learning.</li> <li><strong>Complex Network Architectures</strong>: When building complex architectures such as residual networks (ResNets), inception networks, or U-net architectures for segmentation tasks, the Functional API provides the flexibility to define intricate connections between layers.</li> <li><strong>Models with Branching</strong>: Functional API enables building models with branching, where the network can learn multiple features at different levels of abstraction. This is useful in architectures like Inception networks, where different convolutional filters operate on different scales of features.</li> <li><strong>Models with Skip Connections</strong>: Skip connections are commonly used in architectures like U-net and ResNet to facilitate better gradient flow and alleviate the vanishing gradient problem. Functional API allows easy implementation of skip connections by simply concatenating or adding the outputs of different layers.</li> <li><strong>Custom Model Architectures</strong>: Functional API is suitable for building entirely custom architectures tailored to specific tasks. This includes models for sequence-to-sequence tasks, graph neural networks, attention mechanisms, and more.</li> <li><strong>Model Subclassing</strong>: While the Sequential API does not support dynamic architectures, the Functional API can be combined with model subclassing to create highly customizable models with dynamic behaviors and control flow.</li> <li><strong>Transfer Learning and Fine-tuning</strong>: Functional API provides the flexibility to modify pre-trained models by adding or removing layers, freezing certain layers, or replacing specific layers with custom layers.</li> </ol> <h2 id="build-models-1">Build Models</h2> <p>Letâ€™s build some deep learning models using the functional API.</p> <h3 id="a-simple-neural-networks">A simple neural networks</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Define input layer
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,))</span>
<span class="c1"># Define first hidden layer
</span><span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c1"># Define second hidden layer
</span><span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># Define output layer
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>

<span class="c1"># Compile model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Print model summary
</span><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div> <h3 id="a-complicated-transformer">A complicated Transformer</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">LayerNormalization</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadSelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">projection_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query_dense</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key_dense</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value_dense</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">combine_heads</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">dim_key</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">key</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">scaled_score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">dim_key</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scaled_score</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">separate_heads</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">projection_dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># x.shape = [batch_size, seq_len, embed_dim]
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, embed_dim)
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, embed_dim)
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, embed_dim)
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">separate_heads</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, projection_dim)
</span>        <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">separate_heads</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, projection_dim)
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">separate_heads</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, projection_dim)
</span>        <span class="n">attention</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># (batch_size, seq_len, num_heads, projection_dim)
</span>        <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">))</span>  <span class="c1"># (batch_size, seq_len, embed_dim)
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">combine_heads</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, embed_dim)
</span>        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">att</span> <span class="o">=</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
            <span class="nc">Dense</span><span class="p">(</span><span class="n">ff_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
            <span class="nc">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="nc">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="nc">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout1</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layernorm1</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout2</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layernorm2</span><span class="p">(</span><span class="n">out1</span> <span class="o">+</span> <span class="n">ffn_output</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">build_transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">,</span> <span class="n">input_sequence_length</span><span class="p">,</span> <span class="n">output_sequence_length</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_sequence_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">,</span> <span class="n">rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">output_sequence_length</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>

<span class="c1"># Example usage:
</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">ff_dim</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">input_sequence_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">output_sequence_length</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">transformer</span> <span class="o">=</span> <span class="nf">build_transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ff_dim</span><span class="p">,</span> <span class="n">input_sequence_length</span><span class="p">,</span> <span class="n">output_sequence_length</span><span class="p">)</span>
<span class="n">transformer</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

</code></pre></div></div> <h1 id="other-functionalities">Other Functionalities</h1> <p>Apart from building models, we have a lot of other necessary things to do during deep learning.</p> <h2 id="monitor-training---callback">Monitor Training - Callback</h2> <p>Callbacks in Keras are objects that can perform actions at various stages during training, such as at the start or end of an epoch, before or after a batch, or when a specific performance metric has reached a certain value.</p> <p>They are useful for implementing functionalities:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ModelCheckpoint</code> to save the model with the best validation accuracy</li> <li><code class="language-plaintext highlighter-rouge">EarlyStopping</code> to stop training early if the validation loss does not improve after a certain number of epochs</li> <li><code class="language-plaintext highlighter-rouge">TensorBoard</code> to log training metrics for visualization in TensorBoard</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">TensorBoard</span>

<span class="c1"># Define callbacks
</span>
<span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="nc">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="sh">'</span><span class="s">model_checkpoint.h5</span><span class="sh">'</span><span class="p">,</span> 
                                      <span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">val_accuracy</span><span class="sh">'</span><span class="p">,</span> 
                                      <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                      <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">early_stopping_callback</span> <span class="o">=</span> <span class="nc">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">,</span> 
                                       <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                                       <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                       <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="nc">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">./logs</span><span class="sh">'</span><span class="p">,</span> 
                                   <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                   <span class="n">write_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                   <span class="n">write_images</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">checkpoint_callback</span><span class="p">,</span> <span class="n">early_stopping_callback</span><span class="p">,</span> <span class="n">tensorboard_callback</span><span class="p">]</span>

<span class="c1"># Train the model with callbacks
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> 
          <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
          <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
          <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
</code></pre></div></div> <h2 id="visualize-fitting---training-history">Visualize Fitting - Training History</h2> <p>In Keras, the <code class="language-plaintext highlighter-rouge">fit</code> method returns a <code class="language-plaintext highlighter-rouge">history</code> object containing the training history. This history object records the loss and metrics values at each epoch during the training process. Itâ€™s a valuable resource for analyzing the performance of your model over time and for visualizing the training progress.</p> <p>After training your model using the <code class="language-plaintext highlighter-rouge">fit</code> method, you can access the training history through the <code class="language-plaintext highlighter-rouge">history</code> object. The <code class="language-plaintext highlighter-rouge">history</code> object contains the following attributes:</p> <ul> <li><code class="language-plaintext highlighter-rouge">history.history</code>: A dictionary containing the training and validation metrics recorded at each epoch. The keys of this dictionary are the names of the metrics, and the values are lists containing the corresponding metric values for each epoch.</li> </ul> <p>You can use these metrics to plot graphs and analyze the training progress and the performance of your model. For instance, you can plot the training and validation loss over epochs to <strong>check for overfitting or underfitting</strong>, or plot the accuracy to see <strong>how well your model is learning</strong> from the data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Train the model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> 
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> 
                    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>        <span class="c1"># Training accuracy values
</span><span class="nf">print</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">])</span>            <span class="c1"># Training loss values
</span><span class="nf">print</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">val_accuracy</span><span class="sh">'</span><span class="p">])</span>    <span class="c1"># Validation accuracy values
</span><span class="nf">print</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">])</span>        <span class="c1"># Validation loss values
</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Plot training &amp; validation accuracy values
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">val_accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Model accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Validation</span><span class="sh">'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper left</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plot training &amp; validation loss values
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Model loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Validation</span><span class="sh">'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper left</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2> <p>Performing hyperparameter tuning using Keras models as <strong>scikit-learn estimators</strong> allows us to leverage the powerful tools provided by scikit-learn, such as <strong>GridSearchCV</strong> and <strong>RandomizedSearchCV</strong>, for efficient hyperparameter search. Below is a step-by-step guide on how to achieve this:</p> <ol> <li><strong>Create a Function to Build Keras Models</strong>: First, we define a function that constructs the Keras model. This function will accept hyperparameters as arguments and return a compiled Keras model.</li> <li><strong>Wrap the Keras Model with KerasClassifier</strong>: Next, we use the <code class="language-plaintext highlighter-rouge">KerasClassifier</code> wrapper provided by scikit-learn to turn our Keras model into a scikit-learn estimator. This wrapper allows us to use the Keras model with scikit-learnâ€™s API.</li> <li><strong>Define Hyperparameter Grid</strong>: We define a dictionary specifying the hyperparameters we want to tune and their respective search spaces.</li> <li><strong>Perform Grid Search or Randomized Search</strong>: We use either <code class="language-plaintext highlighter-rouge">GridSearchCV</code> or <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> from scikit-learn to search for the best combination of hyperparameters based on a specified scoring metric (e.g., accuracy or loss).</li> <li><strong>Evaluate and Select the Best Model</strong>: Finally, we evaluate the performance of the best model on the test set and select it for deployment.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="n">keras.wrappers.scikit_learn</span> <span class="kn">import</span> <span class="n">KerasClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Step 1: Create a function to build Keras models
</span><span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">hidden_units</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
        <span class="nc">Dense</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,)),</span>
        <span class="nc">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Step 2: Wrap the Keras model with KerasClassifier
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">KerasClassifier</span><span class="p">(</span><span class="n">build_fn</span><span class="o">=</span><span class="n">create_model</span><span class="p">)</span>

<span class="c1"># Step 3: Define hyperparameter grid
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">optimizer</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sgd</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">activation</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">hidden_units</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Step 4: Perform Grid Search
</span><span class="n">iris</span> <span class="o">=</span> <span class="nf">load_iris</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">grid_result</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Step 5: Evaluate and select the best model
</span><span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_result</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best Parameters:</span><span class="sh">"</span><span class="p">,</span> <span class="n">grid_result</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best Accuracy:</span><span class="sh">"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</code></pre></div></div> <h2 id="using-pre-trained-models">Using Pre-trained Models</h2> <p>Using pretrained models in Keras for prediction tasks is a common practice, especially when dealing with tasks like image classification, object detection, or natural language processing. Keras provides a wide range of pretrained models through its <code class="language-plaintext highlighter-rouge">applications</code> module, including popular architectures like VGG16, ResNet, Inception, and more.</p> <ol> <li><strong>Load the Pretrained Model</strong>: First, load the pretrained model using the appropriate function from <code class="language-plaintext highlighter-rouge">keras.applications</code>.</li> <li><strong>Preprocess Input Data</strong>: Preprocess your input data according to the requirements of the pretrained model. For example, you may need to resize images to match the input size of the model or perform normalization.</li> <li><strong>Make Predictions</strong>: Use the <code class="language-plaintext highlighter-rouge">predict</code> method of the pretrained model to make predictions on your input data.</li> <li><strong>Interpret Predictions</strong>: Interpret the predictions based on the task at hand. For example, in image classification, you may need to map predicted class probabilities to class labels.</li> <li><strong>(Optional) Fine-tuning or Feature Extraction</strong>: If needed, you can fine-tune the pretrained model on your specific dataset or use it as a feature extractor by removing the top layers and adding new ones suited to your task.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import image and preprocess_input
</span><span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.applications.resnet50</span> <span class="kn">import</span> <span class="n">preprocess_input</span>

<span class="c1"># Load the image with the right target size for your model
</span><span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">load_img</span><span class="p">(</span><span class="n">img_path</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>

<span class="c1"># Turn it into an array
</span><span class="n">img_array</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="c1"># Expand the dimensions of the image, this is so that it fits the expected model input format
</span><span class="n">img_expanded</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">img_array</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Pre-process the img in the same way original images were
</span><span class="n">img_ready</span> <span class="o">=</span> <span class="nf">preprocess_input</span><span class="p">(</span><span class="n">img_expanded</span><span class="p">)</span>

<span class="c1"># Instantiate a ResNet50 model with 'imagenet' weights
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="sh">'</span><span class="s">imagenet</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Predict with ResNet50 on your already processed img
</span><span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">img_ready</span><span class="p">)</span>

<span class="c1"># Decode the first 3 predictions
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted:</span><span class="sh">'</span><span class="p">,</span> <span class="nf">decode_predictions</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <h2 id="layer-inspection">Layer Inspection</h2> <p>You can use the <strong>Keras backend</strong> to create a function that computes the output of a specific layer in a Keras model given input data. This can be useful for various purposes, such as debugging, feature extraction, or implementing custom layers or models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import tensorflow.keras backend
</span><span class="kn">import</span> <span class="n">tensorflow.keras.backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="c1"># Input tensor from the 1st layer of the model
</span><span class="n">inp</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nb">input</span>

<span class="c1"># Output tensor from the 1st layer of the model
</span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">output</span>

<span class="c1"># Define a function from inputs to outputs
</span><span class="n">inp_to_out</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="nf">function</span><span class="p">([</span><span class="n">inp</span><span class="p">],</span> <span class="p">[</span><span class="n">out</span><span class="p">])</span>

<span class="c1"># Print the results of passing X_test through the 1st layer
</span><span class="nf">print</span><span class="p">(</span><span class="nf">inp_to_out</span><span class="p">([</span><span class="n">X_test</span><span class="p">]))</span>
</code></pre></div></div> <p>Now, you know how to use Keras Sequential and Functional APIs to build deep learning models such as DNN, CNN, LSTM and Transformer. Also, you know how to babysit your training process. Congratulations!</p>]]></content><author><name></name></author><category term="Learning_Notes"/><category term="Machine_Learning"/><summary type="html"><![CDATA[A comprehensive Keras tutorials with examples]]></summary></entry></feed>